{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c1804cf-7305-412a-b2bf-68d088c17d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "905a1c42-123c-44b2-9d33-239577ec83c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA med\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1972f0c-fd18-4763-8de8-2c785009c53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# load doc_chunks\n",
    "chunks_df = spark.table(\"workspace.med.doc_chunks\")\n",
    "chunks_df = chunks_df.filter(F.col(\"chunk_text\").isNotNull() & (F.length(\"chunk_text\") > 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4805b97-cdbc-464f-ba4f-8c7263715b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert to pandas\n",
    "chunks_pdf = chunks_df.select(\n",
    "    \"chunk_id\",\n",
    "    \"doc_id\",\n",
    "    \"chunk_text\",\n",
    "    \"source\",\n",
    "    \"category\",\n",
    "    \"title\"\n",
    ").toPandas()\n",
    "\n",
    "chunks_pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3a21bb2-9062-4331-9a48-143c23f0bad3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chunk_ids = chunks_pdf[\"chunk_id\"].tolist()\n",
    "texts = chunks_pdf[\"chunk_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7c911b-3b53-4191-ba8d-d7fd0120c14e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "word_vec = TfidfVectorizer(\n",
    "    analyzer=\"word\",\n",
    "    ngram_range=(1, 2),\n",
    "    stop_words=\"english\",\n",
    "    min_df=2,\n",
    "    max_df=0.8,\n",
    "    sublinear_tf=True,\n",
    "    max_features=50000,\n",
    ")\n",
    "\n",
    "char_vec = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",\n",
    "    ngram_range=(3, 5),\n",
    "    min_df=2,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True,\n",
    "    max_features=100000,\n",
    ")\n",
    "\n",
    "vectorizer = FeatureUnion([(\"word\", word_vec), (\"char\", char_vec)])\n",
    "matrix = vectorizer.fit_transform(texts)\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74004f6d-2aed-4115-ac69-977134d3b761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# k nearest neighbour search index and test on the matrix \n",
    "knn = NearestNeighbors(\n",
    "    n_neighbors=10, # max num of neighbours\n",
    "    metric=\"cosine\", # cosine distance\n",
    "    algorithm=\"brute\", # brute force (compare query vector to all vectors)\n",
    "    n_jobs=1\n",
    ").fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "429237c9-b3ba-4e4c-90bd-81a8e150be90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def normalize_text(s: str) -> str:\n",
    "    s = (s or \"\").lower() # ensure input is string\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip() # remove extra spaces\n",
    "    return s\n",
    "\n",
    "# remove duplicated text across all documents\n",
    "def text_hash(s: str) -> str:\n",
    "    return hashlib.md5(normalize_text(s).encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a249d6e-a7b5-4c4b-a75e-393430d3c6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def search_chunks_local(question: str, top_k: int = 5, pool_k: int = 50, max_dist: float = 0.85):\n",
    "    question_vector = vectorizer.transform([question]) # convert question to vector\n",
    "    pool_k = min(pool_k, len(chunks_pdf)) # limit pool size\n",
    "    distances, indices = knn.kneighbors(question_vector, n_neighbors=pool_k) # find nearest neighbors\n",
    "    seen_text = set() # track identical/near-identical chunk text\n",
    "    seen_doc = set() # track document level duplicates\n",
    "    results = []\n",
    "    rank = 0 # ranking counter\n",
    "\n",
    "    for idx, dist in zip(indices[0], distances[0]):\n",
    "        # skip weak matches (higher distance is worse)\n",
    "        if dist > max_dist:\n",
    "            continue\n",
    "\n",
    "        row = chunks_pdf.iloc[int(idx)] # get row from chunks df\n",
    "        doc_id = row.get(\"doc_id\", None) # get doc_id\n",
    "        h = text_hash(row.get(\"chunk_text\") or \"\") # dedupe identical/near-identical chunk text\n",
    "\n",
    "        if h in seen_text:\n",
    "            continue\n",
    "\n",
    "        # dedupe multiple chunks from the same doc\n",
    "        if doc_id is not None and doc_id in seen_doc:\n",
    "            continue\n",
    "        \n",
    "        # mark chunk text as seen\n",
    "        seen_text.add(h)\n",
    "        if doc_id is not None:\n",
    "            seen_doc.add(doc_id)\n",
    "\n",
    "        # increment rank and append the result\n",
    "        rank += 1\n",
    "        results.append({\n",
    "            \"rank\": rank, # ranking position based on cosine distance\n",
    "            \"chunk_id\": row[\"chunk_id\"], \n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": row.get(\"title\"),\n",
    "            \"source\": row.get(\"source\"),\n",
    "            \"category\": row.get(\"category\"),\n",
    "            \"cosine_distance\": float(dist), # cosine distance score\n",
    "            \"cosine_similarity\": float(1.0 - float(dist)), # cosine similarity score\n",
    "            \"chunk_text_preview\": (row.get(\"chunk_text\") or \"\")[:300], # short text preview\n",
    "        })\n",
    "\n",
    "        # stop when there is enough results\n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d74e4c10-8c0e-4918-9c72-3b851a63d262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for r in search_chunks_local(\"what is ibuprofen used for\", top_k=5):\n",
    "    print(r[\"rank\"], r[\"source\"], r[\"title\"], r[\"cosine_distance\"], r[\"chunk_id\"])\n",
    "    print(r[\"chunk_text_preview\"])\n",
    "    print(\"-\" * 100)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "train_tfidf_knn",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
