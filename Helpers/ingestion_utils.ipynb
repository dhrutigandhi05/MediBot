{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc3d79c-2637-4861-a63d-2adb7ae1edab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.functions import lit, current_timestamp, col, split, trim, expr, regexp_replace\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adcf3df-1e34-460c-8244-4df67141c535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CATALOG = \"workspace\"\n",
    "SCHEMA = \"med\"\n",
    "RAW_TABLE = f\"{CATALOG}.{SCHEMA}.raw_data\"\n",
    "\n",
    "RAW_RECORD_SCHEMA = StructType([\n",
    "    StructField(\"doc_id\", StringType(), False),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"synonyms\", StringType(), True),\n",
    "    StructField(\"url\", StringType(), True),\n",
    "    StructField(\"raw_text\", StringType(), True),\n",
    "    StructField(\"meta_json\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2edb49-7b1b-46b2-88d1-27c2779ad737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ensure table exists, if not create it\n",
    "def ensure_raw_table():\n",
    "    create_ddl = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {RAW_TABLE} (\n",
    "      doc_id      STRING,\n",
    "      category    STRING,\n",
    "      source      STRING,\n",
    "      title       STRING,\n",
    "      synonyms    ARRAY<STRING>,\n",
    "      url         STRING,\n",
    "      raw_text    STRING,\n",
    "      meta_json   STRING,\n",
    "      ingested_at TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    \"\"\"\n",
    "    spark.sql(create_ddl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9542c780-c749-485f-b4dd-b4930c8123a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def cast_synonyms_to_array(df: DataFrame, colname: str = \"synonyms\") -> DataFrame:\n",
    "    if colname not in df.columns:\n",
    "        return df.withColumn(colname, lit(None).cast(\"array<string>\"))\n",
    "\n",
    "    dtype = dict(df.dtypes).get(colname)\n",
    "\n",
    "    if dtype and dtype.startswith(\"array\"):\n",
    "        return df.withColumn(colname, col(colname).cast(ArrayType(StringType())))\n",
    "\n",
    "    from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "    df = df.withColumn(colname, regexp_replace(col(colname), \";\", \",\"))\n",
    "    df = df.withColumn(colname, split(col(colname), \",\").cast(\"array<string>\"))\n",
    "    df = df.withColumn(colname, expr(f\"transform({colname}, x -> trim(x))\"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d4da27b-263f-4066-98b3-f6abec04163e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standardize_docs(df: DataFrame, source_value: str) -> DataFrame:\n",
    "    required: List[str] = [\"doc_id\", \"category\", \"title\", \"synonyms\", \"url\", \"raw_text\", \"meta_json\"]\n",
    "\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, lit(None).cast(\"string\"))\n",
    "\n",
    "    df = cast_synonyms_to_array(df, \"synonyms\")\n",
    "\n",
    "    return (\n",
    "        df.select(*required)\n",
    "          .withColumn(\"source\", lit(source_value))\n",
    "          .withColumn(\"ingested_at\", current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e153672-8997-416c-a475-2f141a4f52de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_raw_docs(df: DataFrame):\n",
    "    (df.write.format(\"delta\").mode(\"append\").saveAsTable(RAW_TABLE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7cb706-0735-4417-8125-04b8e68e3709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def overwrite_source(df: DataFrame, source_value: str):\n",
    "    (\n",
    "        df.write\n",
    "          .format(\"delta\")\n",
    "          .mode(\"overwrite\")\n",
    "          .option(\"replaceWhere\", f\"source = '{source_value}'\")\n",
    "          .saveAsTable(RAW_TABLE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323f6908-b8b2-49f7-9bc7-86519b902737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_records_to_raw_data(records: List[dict], source_value: str, preview: bool = False):\n",
    "    if not records:\n",
    "        print(f\"No records to load for source {source_value}\")\n",
    "        return\n",
    "\n",
    "    df = spark.createDataFrame([Row(**r) for r in records], schema=RAW_RECORD_SCHEMA)\n",
    "\n",
    "    if preview:\n",
    "        display(df.limit(5))\n",
    "\n",
    "    standardized = standardize_docs(df, source_value=source_value)\n",
    "    overwrite_source(standardized, source_value=source_value)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingestion_utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
